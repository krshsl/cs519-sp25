--- ./linux-5.15.0/kernel/sched/core.c.orig	2025-05-07 16:58:08.946133446 -0600
+++ ./linux-5.15.0/kernel/sched/core.c	2025-05-08 22:03:18.194053477 -0600
@@ -6,6 +6,10 @@
  *
  *  Copyright (C) 1991-2002  Linus Torvalds
  */
+#include "linux/rbtree_types.h"
+#include "linux/sched.h"
+#include "linux/spinlock.h"
+#include "linux/stddef.h"
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 #undef CREATE_TRACE_POINTS
@@ -27,6 +31,10 @@
 #include "pelt.h"
 #include "smp.h"
 
+#include <linux/my_inactive.h>
+
+DEFINE_PER_CPU(struct inactive_cpu, ia_cpu_queue);
+EXPORT_PER_CPU_SYMBOL(ia_cpu_queue);
 /*
  * Export tracepoints that act as a bare tracehook (ie: have no trace event
  * associated with them) to allow external modules to probe them.
@@ -2088,12 +2096,67 @@
 		p->sched_class->prio_changed(rq, p, oldprio);
 }
 
+
+/*
+ * 1: continue
+ * 0: do not reschedule
+*/
+int is_inactive_preempt(struct rq *rq, struct task_struct *p, int flags) {
+    struct inactive_cpu *cpu;
+    struct inactive_tasks *node = NULL;
+    unsigned long cpu_flags;
+    u64 now;
+
+    if (p->inactive_tracker == -1) {
+        return 1;
+    }
+    cpu = &per_cpu(ia_cpu_queue, p->inactive_cpu); // techincally it should be the same cpu, but why bother
+    now = ktime_get_ns();
+    LOG_FUNCS(KERN_INFO, "Begins... %d::%d", p->inactive_cpu, p->pid);
+
+    if (cpu->counter <= 1) {
+        // if no input?? or only one input, allow preemption
+        return 1;
+    }
+    raw_spin_lock_irqsave(&cpu->lock, cpu_flags);
+    node = list_first_entry(&cpu->proc_list, struct inactive_tasks, head);
+
+    if (node->pid != p->pid) {
+        dequeue_task(rq, p, flags);
+        p->se.vruntime += node->active_time; // it can only exec after a rather long time???
+        enqueue_task(rq, p, flags);
+        raw_spin_unlock_irqrestore(&cpu->lock, cpu_flags);
+        return 0; // do not, task ids are not the same!!!
+    }
+
+    if (now < node->preempt_time) {
+        raw_spin_unlock_irqrestore(&cpu->lock, cpu_flags);
+        return 1; // preemption allowed
+    }
+    list_del(&node->head); // pop and add to back of queue
+    list_add_tail(&node->head, &cpu->proc_list);
+    node = list_first_entry(&cpu->proc_list, struct inactive_tasks, head);
+    node->preempt_time = now+node->active_time; // new head gets updated time
+	raw_spin_unlock_irqrestore(&cpu->lock, cpu_flags);
+	LOG_FUNCS(KERN_INFO, "Ends... %d::%d::%llu", p->inactive_cpu, p->pid, node->preempt_time);
+	dequeue_task(rq, p, flags);
+    p->se.vruntime += 10000LL; // it can only exec again after a min time???
+    enqueue_task(rq, p, flags);
+    raw_spin_unlock_irqrestore(&cpu->lock, cpu_flags);
+	return 0; // preemption is not allowed, let the next task run now
+}
+
 void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 {
+    if (is_inactive_preempt(rq, p, flags) == 0) {
+        return; //do not prempt
+    }
+
 	if (p->sched_class == rq->curr->sched_class)
-		rq->curr->sched_class->check_preempt_curr(rq, p, flags);
+	    rq->curr->sched_class->check_preempt_curr(rq, p, flags);
 	else if (p->sched_class > rq->curr->sched_class)
-		resched_curr(rq);
+	    resched_curr(rq);
+
 
 	/*
 	 * A queue event has occurred, and we're going to schedule.  In
@@ -9546,6 +9609,13 @@
 #endif
 	}
 
+	for_each_possible_cpu(i) {
+        struct inactive_cpu *ia_cpu = &per_cpu(ia_cpu_queue, i);
+        ia_cpu->lock = __RAW_SPIN_LOCK_UNLOCKED(node->lock);
+        ia_cpu->counter = 0;
+        INIT_LIST_HEAD(&ia_cpu->proc_list);
+	}
+
 	set_load_weight(&init_task, false);
 
 	/*
@@ -10981,3 +11051,58 @@
 {
         trace_sched_update_nr_running_tp(rq, count);
 }
+
+SYSCALL_DEFINE3(set_inactive_pid, int, upid, int, task_core, unsigned long long, active_time) {
+    struct task_struct *p;
+    struct inactive_cpu *cpu = NULL;
+    struct inactive_tasks *task = NULL;
+    struct inactive_tasks *node = NULL;
+    struct list_head *pos = NULL;
+    struct list_head *tmp = NULL;
+    unsigned long cpu_flags;
+    struct pid* pid;
+
+    LOG_FUNCS(KERN_INFO, "Begins... %d::%d::%d", current->pid, upid, smp_processor_id());
+    pid = find_vpid(upid);
+    p = pid_task(pid, PIDTYPE_PID);
+    if (p == NULL) {
+        LOG_FUNCS(KERN_INFO, "Proc::%d exited. Ending...", upid);
+        return 0; // process has already exited, no need to add...
+    }
+
+    active_time = max_t(u64, active_time, 20000LL); // active_time should atleast be 2 times the sched val...
+    // active_time minf value is required as well, not adding atm
+    LOG_FUNCS(KERN_INFO, "%d:%d:%d:%llu", current->pid, upid, task_core, active_time);
+    cpu = &per_cpu(ia_cpu_queue, task_core);
+    if (cpu == NULL) { // can never be null cause it is always supposed to exist??
+        LOG_FUNCS(KERN_INFO, "Something seriously broke in %d::%d, but what did???", current->pid, task_core);
+        return -EFAULT;
+    }
+
+    raw_spin_lock_irqsave(&cpu->lock, cpu_flags);
+    list_for_each_safe(pos, tmp, &cpu->proc_list) {
+        node = list_entry(pos, struct inactive_tasks, head);
+    	if (node->pid != upid) continue;
+        task = node;
+    }
+
+    if (task) {
+        LOG_FUNCS(KERN_INFO, "Existing key updated... %d::%d", upid, task_core);
+        task->active_time = active_time;
+        // if we update start_time here, people can abuse this impl
+    } else {
+        LOG_FUNCS(KERN_INFO, "New Key added... %d::%d", upid, task_core);
+        task = kmalloc(sizeof(struct inactive_tasks), GFP_KERNEL);
+        task->pid = upid;
+        task->active_time = active_time;
+        task->preempt_time = ktime_get_ns()+active_time;
+        INIT_LIST_HEAD(&task->head);
+        list_add_tail(&task->head, &cpu->proc_list);
+        cpu->counter++;
+    }
+    p->inactive_tracker = current->pid; // parent pid added to inactive tracker
+    p->inactive_cpu = task_core;
+    raw_spin_unlock_irqrestore(&cpu->lock, cpu_flags);
+    LOG_FUNCS(KERN_INFO, "Ends... %d::%d::%d", current->pid, upid, smp_processor_id());
+    return 0;
+}
